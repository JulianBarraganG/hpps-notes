\chapter{Data as Bits}
\label{chap:bits}

A computer is a machine for processing and transforming information.
Machines inevitably must operate on things that physically exist, so
in order to process information in a machine, we must \emph{represent}
the information in some physical way.  While we stop short of
discussing precisely the physical phenomena that underlie modern
computers, we will look at the notion of \emph{value encodings}---how
mathematical objects can be represented such that they can be
processed by machines.

\section{A Bit}
\label{sec:bit}

\begin{definition}[Bit]
  A bit (\emph{binary digit}) is a logical state that can represent two possible values,
  which we write as $1$ or $0$.
\end{definition}

Conventionally, and in this text, the two possible values are written
$1$ and $0$, as a reference to their interpretation as numbers, but
this merely a question of notation.  We could equally well have used
\emph{true}/\emph{false}, \emph{a}/\emph{b}, \emph{yes}/\emph{no}, or
anything else that allows us to distinguish unambiguously between the
two possible values.  By convention, we say that a bit is \emph{unset}
when $1$ and \emph{unset} when $0$.

Bits are used in information theory as a unit of information.  For
computers, bits are convenient because any physical phenomenon that
can be interpreted as having two states can be used to represent a
bit.  For example:
\begin{enumerate}
\item High or low voltage in an electrical wire.
\item Absence or presence of a hole in some material.
\item Vertical or horizontal polarity of light.
\item Heads or tails of a coin.
\item Whether a cup is full or empty.
\item Whether a corridor is full of soldier crabs or
  not~\cite{gunji2011robust}.
\end{enumerate}
Some of these representations are more practical than others, but all
are ultimately based on the notion of a \emph{bit}.  The choice of
which representation is most practical in a given setting is largely
based on how we can construct machinery that manipulates the physical
representation of the bits.  Modern computers are overwhelmingly
electronic, and use \emph{transistors} to transform electrical signals
representing bits, but optical representations are also common for
long-distance communications.

\subsection{Bit Vectors}

Single bits rarely occur in isolation.  Typically we use a sequence of
bits, called a \emph{bit vector}.

\begin{definition}[Bit vector]
  A bit vector of length $w$ is an ordered sequence of $w$ bits, which
  we write as $\bitvector{x_{w-1}\ldots{}x_{0}}$.
\end{definition}

Note that the convention is that bit $x_{0}$ in a bit vector is
written at the \emph{rightmost} position.  This is again merely a
convention, but done to resemble the ordering of digits in
mathematical notation.

\begin{definition}[Concatenation of bit vectors]
  We denote concatenation with the $\bitconcat$ operator and define it
  the obvious way, as follows:
  \[
    \bitvector{x_{n-1}\ldots{}x_{0}} \bitconcat \bitvector{y_{m-1}\ldots{}y_{0}} =
    \bitvector{x_{n-1}\ldots{}x_{0}~y_{m-1}\ldots{}y_{0}}
  \]
\end{definition}


Bit vectors can be interpreted as encoding various mathematical
objects.  We will initially be representing values that look very
similar to bits, and the following may seem unnecessarily long-winded
and ceremonious, but eventually we will look at more complicated
encodings.  The goal is to establish a firm distinction between the
\emph{encoding} of some mathematical object (say, a number) as a bit
vector, and the mathematical object itself.  Such an encoding is
defined by specifying a pair of \emph{conversion} functions between
the set of bit vectors, which we denote $\mathbb{B}^{*}$, and the
mathematical set we wish to encode, say the natural numbers
$\mathbb{N}$.

\subsection{Bit Vectors as Natural Numbers}
\label{sec:bitnats}

One of the most obvious ways to interpret a bit vector is as a number
in base 2.  For example, the bit vector $\bitvector{1001}$ represents
the number $1001_{2} = 9_{10}$.  Note the subscripts used to denote
the \emph{radix} of the literals---we will include these whenever the
radix would otherwise be ambiguous.  However, it is important to note
that a $\bitvector{1001}$ \emph{is not the same} as $1001_{2}$!  They
are objects in completely different domains, and it makes no sense to
say that they are equal or unequal.  It is merely a quirk of notation
that they look similar when written down, and we shall soon enough see
encodings where this is not the case.

To completely specify the encoding of natural numbers, we must define
how a bit vector of length $w$ is interpreted as a number.  Here we do
treat single bits as numbers, $0$ or $1$, and multiplying them with a
\emph{weight}.

\begin{definition}[Bit vector to natural number]
\begin{align*}
    \mathrm{Bits2N}(\bitvector{x_{w-1}\cdots x_{0}}) \defeq \sum_{i=0}^{w-1} x_{i} \cdot{} 2^{i}
\end{align*}
\label{def:bits2n}
\end{definition}

\begin{example}
  \[
    \begin{array}{rcl}
      \mathrm{Bits2N}(\bitvector{1101}) & = & 1 \cdot 2^{0} + 0 \cdot 2^{1} + 1 \cdot 2^{2} + 1 \cdot 2^{3} \\
                               & = & 13
    \end{array}
  \]
\end{example}

The conversion of a number to a bit vector is slightly less pleasant,
and is defined by the following equations, which together define a
recursive procedure.

\begin{definition}[Natural number to bit vector]
  \begin{align}
    \mathrm{N2Bits}(0) \defeq & \bitvector{0} \\
    \mathrm{N2Bits}(1) \defeq & \bitvector{1} \\
    \mathrm{N2Bits}(2\cdot{}x + 0) \defeq & \mathrm{N2Bits}(x)\bitconcat\bitvector{0} \\
    \mathrm{N2Bits}(2\cdot{}x + 1) \defeq & \mathrm{N2Bits}(x)\bitconcat\bitvector{1}
  \end{align}
\end{definition}

  \begin{example}
\begin{align*}
  \mathrm{N2Bits}(9) &= \mathrm{N2Bits}(2\cdot{}4 + 1) \\
            &= \mathrm{N2Bits}(4) \bitconcat \bitvector{1} \\
            &= \mathrm{N2Bits}(2\cdot{}2 + 0) \bitconcat \bitvector{1} \\
            &= \mathrm{N2Bits}(2) \bitconcat \bitvector{1} \bitconcat \bitvector{1} \\
            &= \mathrm{N2Bits}(2\cdot{}1 + 0) \bitconcat \bitvector{1} \bitconcat \bitvector{1} \\
            &= \mathrm{N2Bits}(1) \bitconcat \bitvector{0} \bitconcat \bitvector{0} \bitconcat \bitvector{1} \\
            &= \bitvector{1} \bitconcat \bitvector{0} \bitconcat \bitvector{0} \bitconcat \bitvector{1} \\
            &= \bitvector{1001}
\end{align*}
  \end{example}

  This representation can express only non-negative numbers, and is
  called \emph{unsigned} for reasons we will discuss in
  \cref{sec:signed-numbers}, where we also treat negative numbers.

  Having an encoding of numbers is not terribly useful unless we can
  also perform \emph{operations}, such as arithmetic, on numbers
  represented in the given encoding.  However, before we can do that,
  we have to talk about Boolean logic.

\section{Boolean Logic}
\label{sec:boolean-logic}

While numbers are one of the most interesting things we can encode as
bit vectors, we will start out by looking at bits as truth values.
\emph{Truth} as a subject of computation was investigated by the
English mathematician George Boole (1815-1864), whose Boolean logic
far predates the modern notions of computers and bits.

Just as we can define operators on \emph{numbers} (such as addition),
we can define operations on \emph{truth values}, writing $T$ for truth
and $F$ for falsity.  For example, \emph{logical-and}, written
$p \land q$, is true if $p$ and $q$ are both true, and otherwise
false, while \emph{logical-and}, written $p \lor q$, is true if either
operand is true.  The \emph{exclusive-or} operation $p \lxor q$ is
true if exactly one of $p$ and $q$ is true, and $\neg p$ is true only
if $p$ is false.  This is shown in \cref{fig:truth-tables}.

\begin{figure}
  \centering

  \begin{displaymath}
\begin{array}{|c c||c|c|c|c|}
p & q & p \land q & p \lor q & p \lxor q & \neg p \\
\hline
T & T & T & T & F & F\\
T & F & F & T & T & F\\
F & T & F & T & T & T\\
F & F & F & F & F & T\\
\end{array}
\end{displaymath}
  \caption{Truth table for \emph{and}, \emph{or}, \emph{exclusive-or}, and negation.}
  \label{fig:truth-tables}
\end{figure}

Since a binary boolean operator can have two possible results ($T$ or
$F$) for a given combination of operands, and there are four possible
combinations of operands, there are $2^{4}=16$ distinct binary
operators on booleans.  Most of these can be written in terms of other
operators.  For example, the \emph{negated-and} operator can be defined as

\begin{align}
  mathrm{nand}(p,q) \defeq \neg (p \land q)
\end{align}

Interestingly, it turns out that the nand operation is universal, in
that \emph{any} boolean function can be written using a combination of
nand operations.  Examples:

\begin{align}
  \neg p &= \mathrm{nand}(p,p) \\
  p \land q &= \mathrm{nand}(\mathrm{nand}(p,q),\mathrm{nand}(p,q)) \\
  p \lor q &= \mathrm{nand}(\mathrm{nand}(p,p),\mathrm{nand}(q,q))
\end{align}

\subsection{Boolean Operations as Bits}

The truth values of Boolean logic can easily be encoded as bits - by
treating $1$ as $T$ and $0$ as $F$, we can apply the boolean operators
directly:

\begin{align}
  0 \land 1 &= 0 \\
  0 \lor 1 &= 1 \\
  0 \lxor 1 &= 1 \\
  \neg 0 &= 1
\end{align}

In \cref{sec:bit} we remarked that bits can be represented using any
physical phenomenon capable of two distinct states.  In order to be
practical for \emph{computation}, we must also be able to easily
implement boolean operations on pairs of bits.  As mentioned above,
the nand operation is universal, so if we can show how to implement
it, we can implement any boolean function.

In practice, it turns out that representation of bits as high and low
voltages makes it easy to implement logical operations with
\emph{transistors}.  It is relatively straightforward to create a
\emph{gate} that has two input wires and one output wire, where the
voltage of the output wire depends on the voltages of the input---and
these gates can be made extremely small using modern manufacturing
techniques.  This is why electronic computers have become the most
popular way of implementing computation.

We will not delve further into how logical operations are physically
implemented.  Instead, we will use the logical operators to build ever
more elaborate operations on bit vectors, knowing that as long as we
can express a computation in terms of bit operations, we can
ultimately express it in hardware.  On this humble foundation we will
build ever more elaborate data representations.

\subsection{Words}
\label{sec:words}

One important concession to practicality is that we will operate on
bit vectors of fixed lengths.  While it is possible to build computers
that operate on bit vectors of arbitrary lengths, it is much more
efficient to build circuits that operate on a fixed number of bits at
a time.  Usually these are powers of two---$8,16,32,64$, etc.  In the
computer systems nomenclature, a bit vector of some directly
hardware-supported fixed size is called a \emph{word}.  Generally,
when we use the term \emph{$w$-bit word}, we mean a bit vector
containing $w$ bits.

A $w$-bit word can express $2^{w}$ different permutations of bits,
meaning it can represent $2^{w}$ different values.  When deciding on a
value representation, deciding how to best exploit this limited range
is important.  In \cref{def:bits2n} we decided that a $w$-bit word can
represent integers in the range $[0,2^{w}-1]$, which certainly feels
intuitive, but we could just as well have defined a conversion
function that encoded integers in the range
$[k,k+2^{w}-1]$\footnote{In fact, this encoding, known as \emph{biased
    numbers} will make in appearance in \cref{sec:floats}}.  We will
also have to decide what happens when an operation produces a value
that lies outside the representable range.

\section{Bit Arithmetic}
\label{sec:bit-arithmetic}

Bit vectors have no inherent meaning.  Though they look like binary
numbers when we write them down on paper, and we saw in
\cref{sec:bitnats} how we can encode natural numbers as bit vectors,
they do not innately ``know'' how to perform arithmetic operations
such as addition.  If we wish to perform an operation on a
mathematical object represented as a bit vector, we must precisely
specify that operation in terms of bit operations.  Our goal is to
specify the operation such that the resulting bit vector, when
decoded, corresponds to the result we would have obtained if we
operated directly on the mathematical objects.  For a mathematical
operation $\odot$, we will write $\BitOdot$ for the equivalent
operation on numbers encoded as bit vectors.

The algorithms for arithmetic we will introduce are largely similar to
those you were hopefully taught for base-10 numbers as a child.  A
large part of learning how to perform binary arithmetic is to
deconstruct what has long since become intuitive and look at the
actual operations we implicitly perform when adding or multiplying
numbers.

\subsection{Addition}
\label{sec:bit-addition}

We wish to define an operation $\BitAdd$ such that
\begin{equation}
  \mathrm{Bits2N}(\mathrm{N2Bits}(y)\BitAdd{}\mathrm{N2Bits}(y)) = x + y
\end{equation}

Adding binary numbers is much like adding decimal numbers.  Starting
from the least significant (rightmost) bits, we add them elementwise,
keeping a carry.  Example for adding $x\BitAdd{} y=s$ where
$x=\bitvector{01011}, y=\bitvector{01001}$:

\begin{center}
\begin{tabular}{l|llrr}
  $i$ & $x_{i}$ & $y_{i}$ & $s_{i}$ & $c_{i}$ \\\hline
  0 & 1 & 1 & 0 & 1 \\
  1 & 1 & 0 & 0 & 1 \\
  2 & 0 & 0 & 1 & 0 \\
  3 & 1 & 1 & 0 & 1 \\
  4 & 0 & 0 & 1 & 0
\end{tabular}
\end{center}

The result is $s=\bitvector{10100}$ with no carry.  In terms of bit
operations, we can express the computation of sums and carries as
follows (recall that $\oplus$ means exclusive-or):
\begin{align}
  s_{0} &= x_{i} \oplus y_{i} \label{eqn:s0} \\
  c_{0} &= x_{i} \land y_{i} \label{eqn:c0} \\
  s_{i} &= x_{i} \oplus y_{i} \oplus c_{i-1} \label{eqn:si} \\
  c_{i} &= (x_{i} \land y_{i})\lor ((x_{i}\lor y_{i})\land c_{i-1}) \label{eqn:ci}
\end{align}

Thus, the definition of $\BitAdd$ is as follows, when adding two
natural numbers represented as a $w$-bit word.
\begin{definition}[Integer addition]
\begin{align*}
  \bitvector{x_{w-1}\cdots{}x_{0}} \BitAdd \bitvector{y_{w-1}\cdots{}y_{0}} \defeq
  \bitvector{s_{w-1}\cdots{}s_{0}}
\end{align*}
where $s_{i},c_{i}$ are as in \cref{eqn:si,eqn:c0,eqn:si,eqn:ci}.
\label{def:intadd}
\end{definition}

Our definition only covers the case where the two operands have the
same number of bits.  We can always \emph{zero-extend} a $w$-bit word
to a $l+w$-bit word by prepending $l$ bits, without changing the
natural number it encodes via $\mathrm{Bits2N}$.

\subsubsection{Overflow}

Our definition of $\BitAdd$ accepts and produces $w$-bit words.  What
happens if the resulting number is larger than $2^{w}-1$ and thus
logically requires more than $w$ bits to be represented?  Following
\cref{def:intadd}, we see that the final carry bit $c_{w-i}$ is not
part of the result---if this bit is $1$, then we say that the addition
has \emph{overflowed}.

\begin{definition}{Integer overflow}
  When the result of an integer operation is so large that it does fit
  in the designated word.
\end{definition}

With the integer representation we have used so far, the result of an
overflow \emph{wraps around} back to zero.  This is conceptually
similar to adding $5+5$ in decimal arithmetic and limiting the result
to a single digit.

\begin{example}[$\bitvector{11} \BitAdd \bitvector{10}$]
  \begin{align*}
    s_{0} &= 1 \oplus 0 &= 1 \\
    c_{0} &= 1 \land 0 &= 0 \\
    s_{1} &= 1 \oplus 1 \oplus 0 &= 0 \\
    c_{1} &= (1 \land 1) \lor ((1\lor 0) \land 0) &= 1 \\
  \end{align*}
  This gives us the final sum
  \begin{align*}
    \bitvector{11} \BitAdd \bitvector{10} = \bitvector{01}
  \end{align*}
  and since $c_{1}$ is set, the computation has overflowed.
\end{example}

Some programming languages make the last carry bit available as an
\emph{overflow bit} that programmers can check to see if overflow
occurred.  In others, an error is signalled if the overflow bit is set
after an addition.  But in many languages, such as C, overflow
silently occurs and means the program can produce a possibly
unexpected result when working with large numbers.

Does this mean that our carefully specified encoding of natural
numbers as a fixed quantity of bits is simply mathematically wrong,
when even something as simple as addition can give us an unexpected
result?  Not exactly: while no fixed-size encoding can encompass the
infinite natural numbers, our encoding encoding models the \emph{ring
  of natural numbers modulo $2^{w}$}.  Our definition of arithmetic is
\emph{modular arithmetic}, which is mathematically quite well behaved.
In particular, arithmetic has the expected algebraic properties
(associativity, commutativity, $0$ as additive identity, etc).

\subsection{Multiplication}
\label{sec:bit-multiplication}

One useful property of our representation is that we can multiply a
number by $2$ by appending $\bitvector{0}$.  This is analogous to
multiplying a decimal integer by $10$ by appending $0$.  As we will
need it below, we define a shorthand for multiplying a $w$-bit word
with a constant power of two.
\begin{definition}[Integer multiplication by $2^{k}$]
  \begin{align*}
  \bitvector{x_{w-1}\cdots{}x_{0}} << k \defeq
  \bitvector{x_{w-1-k}\cdots{}x_{0},\underbrace{0,\ldots,0}_{k}}
  \end{align*}
  \label{def:mulpow2}
\end{definition}
Note that like addition, this is susceptible to overflow, as we
discard the original $k$ leftmost bits.

General multiplication is a bit more involved, but can be done using
essentially the same algorithm you learned in elementary school, only
with bits instead of digits.  More efficient algorithms exist, but are
much more complicated.  The formula for computing the product
$z=x\times{}y$ is
\begin{equation}
  z = \sum_{i=0}^{k-1} (x \times y_{i}) \times 2^{i}
\end{equation}
where $y_{i}$ is the $i$th bit of $y$.  Note:
\begin{itemize}
\item The product $x \cdot y_{i}$ is multiplying a number with a
  single bit, meaning the result is either $x$ or $0$, which we can
  compute by using a logical-and on every bit of $x$.
\item The multiplication with $2^{i}$ is with a power of 2, which we
  saw how to do in \cref{def:mulpow2}.
\end{itemize}

This lets us express the formula in terms of bit operations.

\begin{definition}[Integer multiplication]
\begin{align*}
  \bitvector{x_{w-1}\cdots{}x_{0}} \BitMul \bitvector{y_{w-1}\cdots{}y_{0}} \defeq
  \sum_{i=0}^{w-1} \bitvector{x_{w-1} \land y_{i},\ldots,x_{0} \land y_{i}} << i
\end{align*}
where summation is with $\BitAdd$.
\end{definition}

\begin{example}[$5 \times 3$]
  \begin{align*}
    \bitvector{0101} \BitMul \bitvector{0011} &=& \sum_{i=0}^{3} \bitvector{0 \land y_{i},1\land y_{i}, 0\land y_{i}, 1 \land y_{i}} << i \\
                                              &=& \bitvector{0 \land 1,1\land 1, 0\land 1, 1 \land 1} << 0 \\
                                              &&\BitAdd \bitvector{0 \land 1,1\land 1, 0\land 1, 1 \land 1} << 1 \\
                                              &&\BitAdd \bitvector{0 \land 0,1\land 0, 0\land 0, 1 \land 0} << 2 \\
                                              &&\BitAdd \bitvector{0 \land 0,1\land 0, 0\land 0, 1 \land 0} << 3 \\
                                              &=&         (\bitvector{0101} << 0) \\
    &&\BitAdd (\bitvector{0101} << 1) \\
    && \BitAdd (\bitvector{0000} << 2) \\
                                              && \BitAdd (\bitvector{0000} << 3) \\
                                              &=& \bitvector{0101} \BitAdd \bitvector{1010} \BitAdd \bitvector{0000}  \BitAdd \bitvector{0000} \\
    &=& \bitvector{1111}
  \end{align*}
\end{example}

\section{Signed Numbers}
\label{sec:signed-numbers}

The number representation discussed thus far can only represent
non-negative numbers, which in computer science jargon are called
\emph{unsigned}.  If we want to handle negative numbers as well, we
need a \emph{signed} representation.

\subsection{Sign-magnitude}
\label{sec:sign-magnitude}

In normal number notation, we turn a number negative by prefixing it
with a minus sign.  Thus, an obvious way to introduce negative numbers
is to treat the most significant (leftmost) bit as a \emph{sign bit},
which is set when the number is negative.  This representation is
known as \emph{sign-magnitude}.  The conversion function is as
follows:

\begin{definition}[Bit vector to integer using sign-magnitude]
\begin{align*}
  \mathrm{SM2Int}(\bitvector{x_{w-1}\cdots x_{0}}) \defeq -1x_{w-1} \times \mathrm{Bits2N}(\bitvector{x_{w-2}\cdots x_{0}})
\end{align*}
\label{def:sm2int}
\end{definition}

Negating a number in sign-magnitude representation is quite simple:
just flip the sign bit.

\begin{table}
  \centering
  \[
  \begin{array}{c|r||c|r}
    x & \mathrm{SM2Int}(x) & x & \mathrm{SM2Int}(x) \\\hline
    \bitvector{0000} & 0 & \bitvector{1000} & 0 \\
    \bitvector{0001} & 1 & \bitvector{1001} & -1 \\
    \bitvector{0010} & 2 & \bitvector{1010} & -2 \\
    \bitvector{0011} & 3 & \bitvector{1011} & -3 \\
    \bitvector{0100} & 4 & \bitvector{1100} & -4 \\
    \bitvector{0101} & 5 & \bitvector{1101} & -5 \\
    \bitvector{0110} & 6 & \bitvector{1110} & -6 \\
    \bitvector{0111} & 7 & \bitvector{1111} & -7 \\
  \end{array}
  \]
  \caption{All possible four-bit words interpreted as integers using
    sign-magnitude representation.}
  \label{tab:sign-mag}
\end{table}

Although simple, this representation has the downside that it contains
two representations of zero, as seen in \cref{tab:sign-mag}.  While
having multiple representations of the same value is not inherently
wrong, it is slightly wasteful, and complicates the definition of
arithmetic.  In particular, arithmetic and comparisons of
sign-magnitude numbers is somewhat involved because we have to treat
the sign bit specially.  For these reasons, sign-magnitude is not used
for integers in most modern computers.

\subsection{Two's complement}
\label{sec:twos-complement}

The overwhelmingly most common integer representation in modern
computers is known as \emph{two's complement}.  In this
representation, a negative number is encoded by the logically negated
bit sequence of the corresponding positive number, plus one.  Another
view is that Two's Complement represents integers by assigning each
bit a weight, just like with unsigned numbers, but assigns the most
significant bit (the \emph{sign bit}) a large \emph{negative} weight.

\begin{definition}[Two's Complement to integer]
\begin{align*}
  \mathrm{TC2N}(\bitvector{x_{w-1}\cdots x_{0}}) \defeq -x_{w-1}\cdot 2^{w-1} + \sum_{i=0}^{w-2} x_{i} \cdot{} 2^{i}
\end{align*}
\label{def:tc2n}
\end{definition}

\begin{table}
  \centering
  \[
  \begin{array}{c|r||c|r}
    x & \mathrm{TC2Int}(x) & x & \mathrm{TC2Int}(x) \\\hline
    \bitvector{0000} & 0 & \bitvector{1000} & -8 \\
    \bitvector{0001} & 1 & \bitvector{1001} & -7 \\
    \bitvector{0010} & 2 & \bitvector{1010} & -6 \\
    \bitvector{0011} & 3 & \bitvector{1011} & -5 \\
    \bitvector{0100} & 4 & \bitvector{1100} & -4 \\
    \bitvector{0101} & 5 & \bitvector{1101} & -3 \\
    \bitvector{0110} & 6 & \bitvector{1110} & -2 \\
    \bitvector{0111} & 7 & \bitvector{1111} & -1 \\
  \end{array}
  \]
  \caption{All possible four-bit words interpreted as integers using
    Two's Complement representation.}
  \label{tab:twos-complement}
\end{table}

Using Two's Complement, all distinct bit vectors represent distinct
integers, as demonstrated on \cref{tab:twos-complement}.  But this
representation also has other useful properties.  One almost
miraculous property is that we can add and multiply numbers in Two's
Complement representation the exact same way that we add unsigned
numbers using \cref{def:intadd}, and get the right result (ignoring
overflow).  For non-negative numbers, this is not terribly surprising,
as these have identical representations in unsigned and Two's
Complement representation.  The reason is that negative numbers in
Two's Complement maintain their relative ordering when interpreted as
unsigned numbers, which is not the case for sign-magnitude.  It is
likely that it is this property that has made Two's Complement the
dominant integer representation, as it means a computer designer can
use the same circuitry for processing signed and unsigned numbers.

Negation of Two's Complement numbers is done by flipping all the bits
and then incrementing by one:

\begin{definition}[Negating a Two's Complement number]
  \[
    \mathrm{negTC}(\bitvector{x_{w-1}\cdots{}x_{0}}) \defeq \bitvector{\neg{}x_{w-1}\cdots{}\neg{}x_{0}} \BitAdd \bitvector{0\cdots{}1}
  \]
\end{definition}

However, because the range of Two's Complement is asymmetric---there
are more negative than positive numbers---negation of the most
negative number is an identity operation.

\begin{example}[Overflow when negating]
  \begin{align*}
    \mathrm{negTC}(\bitvector{1000})
    &= \bitvector{\neg 1 \neg 0 \neg 0 \neg 0} \BitAdd \bitvector{0001} \\
    &= \bitvector{0 1 1 1} \BitAdd \bitvector{0001} \\
    &= \bitvector{1 0 0 0} \\
  \end{align*}
\end{example}

Nevertheless, the definition of negation is sufficient for us to
define subtraction of numbers in Two's Complement:

\begin{definition}[Integer subtraction in Two's Complement]
  \begin{align*}
    x \BitSub y \defeq x \BitAdd \mathrm{negTC}(y)
  \end{align*}
  \label{def:intsub}
\end{definition}

\subsection{Sign Extension}
\label{sec:sign-extension}

In \cref{sec:bit-addition} we saw that a $w$-bit unsigned number can
be extended to a $w+k$-bit number by prepending zeroes.  Such
\emph{zero extension} can however change the numeric interpretation of
a Two's Complement number.

\begin{example}[Zero extension may change value]
  \begin{align*}
    \mathrm{TC2Int}(\bitvector{1100}) &= -4 \\
    \mathrm{TC2Int}(\bitvector{00001100}) &= 12 \\
    \mathrm{TC2Int}(\bitvector{0100}) &= 4 \\
    \mathrm{TC2Int}(\bitvector{00000100}) &= 4
  \end{align*}
\end{example}

To extend a $w$-bit word to a $w-k$-bit word while preserving its
Two's Complement numeric value, we perform \emph{sign extension} where
we prepend copies of the sign bit.

\begin{example}[Sign extension preserves value]
  \begin{align*}
    \mathrm{TC2Int}(\bitvector{1100}) &= -4 \\
    \mathrm{TC2Int}(\bitvector{11111100}) &= -4 \\
    \mathrm{TC2Int}(\bitvector{0100}) &= 4 \\
    \mathrm{TC2Int}(\bitvector{00000100}) &= 4 \\
  \end{align*}
\end{example}

\section{Floating-Point Numbers}
\label{sec:floats}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "notes"
%%% End:
