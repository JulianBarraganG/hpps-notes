\chapter{Data as Bits}
\label{chap:bits}

A computer is a machine for transforming information.  Machines
inevitably must operate on things that physically exist, so in order
to process information in a machine, we must \emph{represent} the
information in some physical way.  While we stop short of discussing
precisely the physical phenomena that underlie modern computers, we
will look at the notion of \emph{value encodings}---how mathematical
objects can be represented such that they can be processed by
machines.

\section{A Bit}
\label{sec:bit}

\begin{definition}[Bit]
  A bit (\emph{binary digit}) is a logical state that can represent two possible values,
  which we write as $1$ or $0$.
\end{definition}

Conventionally, and in this text, the two possible values are written
$1$ and $0$, as a reference to their interpretation as numbers, but
this merely a question of notation.  We could equally well have used
\emph{true}/\emph{false}, \emph{a}/\emph{b}, \emph{yes}/\emph{no}, or
anything else that allows us to distinguish unambiguously between the
two possible values.  By convention, we say that a bit is \emph{unset}
when $1$ and \emph{unset} when $0$.

Bits are used in information theory as a unit of information.  For
computers, bits are convenient because any physical phenomenon that
can be interpreted as having two states can be used to represent a
bit.  For example:
\begin{enumerate}
\item High or low voltage in an electrical wire.
\item Absence or presence of a hole in some material.
\item Vertical or horizontal polarity of light.
\item Heads or tails of a coin.
\item Whether a cup is full or empty.
\item Whether a corridor is full of soldier crabs or
  not~\cite{gunji2011robust}.
\end{enumerate}
Some of these representations are more practical than others, but all
are ultimately based on the notion of a \emph{bit}.  The choice of
which representation is most practical in a given setting is largely
based on how we can construct machinery that manipulates the physical
representation of the bits.  Modern computers are overwhelmingly
electronic, and use \emph{transistors} to transform electrical signals
representing bits, but optical representations are also common for
long-distance communications.

\subsection{Bit Vectors}

\newcommand\bitvector[1]{\langle #1 \rangle}
\newcommand\bitconcat{\frown}

Single bits rarely occur in isolation.  Typically we use a sequence of
bits, called a \emph{bit vector}.

\begin{definition}[Bit vector]
  A bit vector of length $w$ is an ordered sequence of $w$ bits, which
  we write as $\bitvector{x_{w-1}\ldots{}x_{0}}$.
\end{definition}

Note that the convention is that bit $x_{0}$ in a bit vector is
written at the \emph{rightmost} position.  This is again merely a
convention, but done to resemble the ordering of digits in
mathematical notation.

\begin{definition}[Concatenation of bit vectors]
  We denote concatenation with the $\bitconcat$ operator and define it
  the obvious way, as follows:
  \[
    \bitvector{x_{n-1}\ldots{}x_{0}} \bitconcat \bitvector{y_{m-1}\ldots{}y_{0}} =
    \bitvector{x_{n-1}\ldots{}x_{0}~y_{m-1}\ldots{}y_{0}}
  \]
\end{definition}


Bit vectors can be interpreted as encoding various mathematical
objects.  We will initially be representing values that look very
similar to bits, and the following may seem unnecessarily long-winded
and ceremonious, but eventually we will look at more complicated
encodings.  The goal is to establish a firm distinction between the
\emph{encoding} of some mathematical object (say, a number) as a bit
vector, and the mathematical object itself.  Such an encoding is
defined by specifying a pair of \emph{conversion} functions between
the set of bit vectors, which we denote $\mathbb{B}^{*}$, and the
mathematical set we wish to encode, say the natural numbers
$\mathbb{N}$.

\subsection{Bit Vectors as Natural Numbers}
\label{sec:bitnats}

One of the most obvious ways to interpret a bit vector is as a number
in base 2.  For example, the bit vector $\bitvector{1001}$ represents
the number $1001_{2} = 9_{10}$.  Note the subscripts used to denote
the \emph{radix} of the literals---we will include these whenever the
radix would otherwise be ambiguous.  However, it is important to note
that a $\bitvector{1001}$ \emph{is not the same} as $1001_{2}$!  They
are objects in completely different domains, and it makes no sense to
say that they are equal or unequal.  It is merely a quirk of notation
that they look similar when written down, and we shall soon enough see
encodings where this is not the case.

To completely specify the encoding of natural numbers, we must define
how a bit vector of length $w$ is interpreted as a number.  Here we do
treat single bits as numbers, $0$ or $1$, and multiplying them with a
\emph{weight}.

\begin{definition}[Bit vector to natural number]
\begin{align*}
    Bits2N(\bitvector{x_{w-1}\cdots x_{0}}) \defeq \sum_{i=0}^{w} x_{i} \cdot{} 2^{i}
\end{align*}
\label{def:bits2n}
\end{definition}

\begin{example}
  \[
    \begin{array}{rcl}
      Bits2N(\bitvector{1101}) & = & 1 \cdot 2^{0} + 0 \cdot 2^{1} + 1 \cdot 2^{2} + 1 \cdot 2^{3} \\
                               & = & 13
    \end{array}
  \]
\end{example}

The conversion of a number to a bit vector is slightly less pleasant,
and is defined by the following equations, which together define a
recursive procedure.

\begin{definition}[Natural number to bit vector]
  \begin{align}
    N2Bits(0) \defeq & \bitvector{0} \\
    N2Bits(1) \defeq & \bitvector{1} \\
    N2Bits(2\cdot{}x + 0) \defeq & N2Bits(x)\bitconcat\bitvector{0} \\
    N2Bits(2\cdot{}x + 1) \defeq & N2Bits(x)\bitconcat\bitvector{1}
  \end{align}
\end{definition}

  \begin{example}
\begin{align*}
  N2Bits(9) &= N2Bits(2\cdot{}4 + 1) \\
            &= N2Bits(4) \bitconcat \bitvector{1} \\
            &= N2Bits(2\cdot{}2 + 0) \bitconcat \bitvector{1} \\
            &= N2Bits(2) \bitconcat \bitvector{1} \bitconcat \bitvector{1} \\
            &= N2Bits(2\cdot{}1 + 0) \bitconcat \bitvector{1} \bitconcat \bitvector{1} \\
            &= N2Bits(1) \bitconcat \bitvector{0} \bitconcat \bitvector{0} \bitconcat \bitvector{1} \\
            &= \bitvector{1} \bitconcat \bitvector{0} \bitconcat \bitvector{0} \bitconcat \bitvector{1} \\
            &= \bitvector{1001}
\end{align*}
  \end{example}

  This representation can express only non-negative numbers.  We will
  look at how to handle negative numbers in \cref{sec:signed-numbers}.
  Having an encoding of numbers is not terribly useful unless we can
  also perform \emph{operations}, such as arithmetic, on numbers
  represented in the given encoding.  However, before we can do that,
  we have to talk about Boolean logic.

\section{Boolean Logic}
\label{sec:boolean-logic}

While numbers are one of the most interesting things we can encode as
bit vectors, we will start out by looking at bits as truth values.
\emph{Truth} as a subject of computation was investigated by the
English mathematician George Boole (1815-1864), whose Boolean logic
far predates the modern notions of computers and bits.

Just as we can define operators on \emph{numbers} (such as addition),
we can define operations on \emph{truth values}, writing $T$ for truth
and $F$ for falsity.  For example, \emph{logical-and}, written
$p \land q$, is true if $p$ and $q$ are both true, and otherwise
false, while \emph{logical-and}, written $p \lor q$, is true if either
operand is true.  The \emph{exclusive-or} operation $p \lxor q$ is
true if exactly one of $p$ and $q$ is true, and $\neg p$ is true only
if $p$ is false.  This is shown in \cref{fig:truth-tables}.

\begin{figure}
  \centering

  \begin{displaymath}
\begin{array}{|c c||c|c|c|c|}
p & q & p \land q & p \lor q & p \lxor q & \neg p \\
\hline
T & T & T & T & F & F\\
T & F & F & T & T & F\\
F & T & F & T & T & T\\
F & F & F & F & F & T\\
\end{array}
\end{displaymath}
  \caption{Truth table for \emph{and}, \emph{or}, \emph{exclusive-or}, and negation.}
  \label{fig:truth-tables}
\end{figure}

Since a binary boolean operator can have two possible results ($T$ or
$F$) for a given combination of operands, and there are four possible
combinations of operands, there are $2^{4}=16$ distinct binary
operators on booleans.  Most of these can be written in terms of other
operators.  For example, the \emph{negated-and} operator can be defined as

\begin{align}
  nand(p,q) \defeq \neg (p \land q)
\end{align}

Interestingly, it turns out that the nand operation is universal, in
that \emph{any} boolean function can be written using a combination of
nand operations.  Examples:

\begin{align}
  \neg p &= nand(p,p) \\
  p \land q &= nand(nand(p,q),nand(p,q)) \\
  p \lor q &= nand(nand(p,p),nand(q,q))
\end{align}

\subsection{Boolean Operations as Bits}

The truth values of Boolean logic can easily be encoded as bits - by
treating $1$ as $T$ and $0$ as $F$, we can apply the boolean operators
directly:

\begin{align}
  0 \land 1 &= 0 \\
  0 \lor 1 &= 1 \\
  0 \lxor 1 &= 1 \\
  \neg 0 &= 1
\end{align}

In \cref{sec:bit} we remarked that bits can be represented using any
physical phenomenon capable of two distinct states.  In order to be
practical for \emph{computation}, we must also be able to easily
implement boolean operations on pairs of bits.  As mentioned above,
the nand operation is universal, so if we can show how to implement
it, we can implement any boolean function.

In practice, it turns out that representation of bits as high and low
voltages makes it easy to implement logical operations with
\emph{transistors}.  It is relatively straightforward to create a
\emph{gate} that has two input wires and one output wire, where the
voltage of the output wire depends on the voltages of the input---and
these gates can be made extremely small using modern manufacturing
techniques.  This is why electronic computers have become the most
popular way of implementing computation.

We will not delve further into how logical operations are physically
implemented.  Instead, we will use the logical operators to build ever
more elaborate operations on bit vectors, knowing that as long as we
can express a computation in terms of bit operations, we can
ultimately express it in hardware.  On this humble foundation we will
build ever more elaborate data representations.

\subsection{Words}
\label{sec:words}

One important concession to practicality is that we will operate on
bit vectors of fixed lengths.  While it is possible to build computers
that operate on bit vectors of arbitrary lengths, it is much more
efficient to build circuits that operate on a fixed number of bits at
a time.  Usually these are powers of two---$8,16,32,64$, etc.  In the
computer systems nomenclature, a bit vector of some directly
hardware-supported fixed size is called a \emph{word}.  Generally,
when we use the term \emph{$w$-bit word}, we mean a bit vector
containing $w$ bits.

A $w$-bit word can express $2^{w}$ different permutations of bits,
meaning it can represent $2^{w}$ different values.  When deciding on a
value representation, deciding how to best exploit this limited range
is important.  In \cref{def:bits2n} we decided that a $w$-bit word can
represent integers in the range $[0,2^{w}-1]$, which certainly feels
intuitive, but we could just as well have defined a conversion
function that encoded integers in the range
$[k,k+2^{w}-1]$\footnote{In fact, this encoding, known as \emph{biased
    numbers} will make in appearance in \cref{sec:floats}}.  We will
also have to decide what happens when an operation produces a value
that lies outside the representable range.

\section{Bit Arithmetic}
\label{sec:bit-arithmetic}

Bit vectors have no inherent meaning.  Though they look like binary
numbers when we write them down on paper, and we saw in
\cref{sec:bitnats} how we can encode natural numbers as bit vectors,
they do not innately ``know'' how to perform arithmetic operations
such as addition.  If we wish to perform an operation on a
mathematical object represented as a bit vector, we must precisely
that operation in terms of bit operations.  Our goal is to specify the
operation such that the resulting bit vector, when decoded,
corresponds to the result we would have obtained if we operated
directly on the mathematical objects.  For a mathematical operation
$\odot$, we will write $\BitOdot$ for the equivalent operation on
numbers encoded as bit vectors.

The algorithms for arithmetic we will introduce are largely similar to
those you were hopefully taught for base-10 numbers as a child.  A
large part of learning how to perform binary arithmetic is to
deconstruct what has long since become intuitive and look at the
actual operations we implicitly perform when adding or multiplying
numbers.

\subsection{Addition}
\label{sec:bit-addition}

We wish to define an operation $\BitAdd$ such that
\begin{equation}
  Bits2N(N2Bits(y)\BitAdd{}N2Bits(y)) = x + y
\end{equation}

Adding binary numbers is much like adding decimal numbers.  Starting
from the least significant (rightmost) bits, we add them elementwise,
keeping a carry.  Example for adding $x\BitAdd{} y=s$ where
$x=\bitvector{01011}, y=\bitvector{01001}$:

\begin{center}
\begin{tabular}{l|llrr}
  $i$ & $x_{i}$ & $y_{i}$ & $s_{i}$ & $c_{i}$ \\\hline
  0 & 1 & 1 & 0 & 1 \\
  1 & 1 & 0 & 0 & 1 \\
  2 & 0 & 0 & 1 & 0 \\
  3 & 1 & 1 & 0 & 1 \\
  4 & 0 & 0 & 1 & 0
\end{tabular}
\end{center}

The result is $s=\langle{11110}$ with no carry.  In terms of bit
operations, we can express the computation of sums and carries as
follows (recall that $\oplus$ means exclusive-or):
\begin{align}
  s_{0} &= x_{i} \oplus y_{i} \label{eqn:s0} \\
  c_{0} &= x_{i} \land y_{i} \label{eqn:c0} \\
  s_{i} &= x_{i} \oplus y_{i} \oplus c_{i-1} \label{eqn:si} \\
  c_{i} &= (x_{i} \land y_{i})\lor ((x_{i}\lor y_{i})\land c_{i-1}) \label{eqn:ci}
\end{align}

Thus, the definition of $\BitAdd$ is as follows, when adding two
natural numbers represented as $w$-bit word.
\begin{definition}[Integer addition]
\begin{align*}
  \bitvector{x_{w-1}\cdots{}x_{0}} \BitAdd \bitvector{y_{w-1}\cdots{}y_{0}} \defeq
  \bitvector{s_{w-1}\cdots{}s_{0}}
\end{align*}
where $s_{i},c_{i}$ are as in \cref{eqn:si,eqn:c0,eqn:si,eqn:ci}.
\label{def:intadd}
\end{definition}

Our definition only covers the case where the two operands have the
same number of bits.  We can always \emph{zero-extend} a $w$-bit word
to a $l+w$-bit word by prepending $l$ bits, without changing the
natural number it encodes via $Bits2N$.

\subsubsection{Overflow}

Our definition of $\BitAdd$ accepts and produces $w$-bit words.  What
happens if the resulting number is larger than $2^{w}-1$ and thus
logically requires more than $w$ bits to be represented?  Following
\cref{def:intadd}, we see that the final carry bit $c_{w-i}$ is not
part of the result---if this bit is $1$, then we say that the addition
has \emph{overflowed}.

\begin{definition}{Integer overflow}
  When the result of an integer operation is so large that it does fit
  in the designated word.
\end{definition}

With the integer representation we have used so far, the result of an
overflow \emph{wraps around} back to zero.  This is conceptually
similar to adding $5+5$ in decimal arithmetic and limiting the result
to a single digit.

\begin{example}[$\bitvector{11} \BitAdd \bitvector{10}$]
  \begin{align*}
    s_{0} &= 1 \oplus 0 &= 1 \\
    c_{0} &= 1 \land 0 &= 0 \\
    s_{1} &= 1 \oplus 1 \oplus 0 &= 0 \\
    c_{1} &= (1 \land 1) \lor ((1\lor 0) \land 0) &= 1 \\
  \end{align*}
  This gives us the final sum
  \begin{align*}
    \bitvector{11} \BitAdd \bitvector{10} = \bitvector{01}
  \end{align*}
  and since $c_{1}$ is set, the computation has overflowed.
\end{example}

Some programming languages make the last carry bit available as an
\emph{overflow bit} that programmers can check to see if overflow
occurred.  In others, an error is signalled if the overflow bit is set
after an addition.  But in many languages, such as C, overflow
silently occurs and means the program can produce a possibly
unexpected result when working with large numbers.

Does this mean that our carefully specified encoding of natural
numbers as a fixed quantity of bits is simply mathematically wrong,
when even something as simple as addition can give us an unexpected
result?  Not exactly: while no fixed-size encoding can encompass the
infinite natural numbers, our encoding encoding models the \emph{ring
  of natural numbers modulo $2^{w}$}.  Our definition of arithmetic is
\emph{modular arithmetic}, which is mathematically quite well behaved.
In particular, arithmetic has the expected algebraic properties
(associativity, commutativity, $0$ as additive identity, etc).

\subsection{Multiplication}
\label{sec:bit-multiplication}

One useful property of our representation is that we can multiply a
number by $2$ by appending $\bitvector{0}$.  This is analogous to
multiplying a decimal integer by $10$ by appending $0$.  As we will
need it below, we define a shorthand for multiplying a $w$-bit word
with a constant power of two.
\begin{definition}[Integer multiplication by $2^{k}$]
  \begin{align*}
  \bitvector{x_{w-1}\cdots{}x_{0}} << k \defeq
  \bitvector{x_{w-1-k}\cdots{}x_{0},\underbrace{0,\ldots,0}_{k}}
  \end{align*}
  \label{def:mulpow2}
\end{definition}
Note that like addition, this is susceptible to overflow, as we
discard the original $k$ leftmost bits.

General multiplication is a bit more involved, but can be done using
essentially the same algorithm you learned in elementary school, only
with bits instead of digits.  More efficient algorithms exist, but are
much more complicated.  The formula for computing the product
$z=x\times{}y$ is
\begin{equation}
  z = \sum_{i=0}^{k-1} (x \times y_{i}) \times 2^{i}
\end{equation}
where $y_{i}$ is the $i$th bit of $y$.  Note:
\begin{itemize}
\item The product $x \cdot y_{i}$ is multiplying a number with a
  single bit, meaning the result is either $x$ or $0$, which we can
  compute by using a logical-and on every bit of $x$.
\item The multiplication with $2^{i}$ is with a power of 2, which we
  saw how to do in \cref{def:mulpow2}.
\end{itemize}

This lets us express the formula in terms of bit operations.

\begin{definition}[Integer multiplication]
\begin{align*}
  \bitvector{x_{w-1}\cdots{}x_{0}} \BitMul \bitvector{y_{w-1}\cdots{}y_{0}} \defeq
  \sum_{i=0}^{w-1} \bitvector{x_{w-1} \land y_{i},\ldots,x_{0} \land y_{i}} << i
\end{align*}
where summation is with $\BitAdd$.
\end{definition}

\begin{example}[$5 \times 3$]
  \begin{align*}
    \bitvector{0101} \BitMul \bitvector{0011} &=& \sum_{i=0}^{3} \bitvector{0 \land y_{i},1\land y_{i}, 0\land y_{i}, 1 \land y_{i}} << i \\
                                              &=& \bitvector{0 \land 1,1\land 1, 0\land 1, 1 \land 1} << 0 \\
                                              &&\BitAdd \bitvector{0 \land 1,1\land 1, 0\land 1, 1 \land 1} << 1 \\
                                              &&\BitAdd \bitvector{0 \land 0,1\land 0, 0\land 0, 1 \land 0} << 2 \\
                                              &&\BitAdd \bitvector{0 \land 0,1\land 0, 0\land 0, 1 \land 0} << 3 \\
                                              &=&         (\bitvector{0101} << 0) \\
    &&\BitAdd (\bitvector{0101} << 1) \\
    && \BitAdd (\bitvector{0000} << 2) \\
                                              && \BitAdd (\bitvector{0000} << 3) \\
                                              &=& \bitvector{0101} \BitAdd \bitvector{1010} \BitAdd \bitvector{0000}  \BitAdd \bitvector{0000} \\
    &=& \bitvector{1111}
  \end{align*}
\end{example}

\section{Signed Numbers}
\label{sec:signed-numbers}

\section{Floating-Point Numbers}
\label{sec:floats}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "notes"
%%% End:
